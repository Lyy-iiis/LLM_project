## 24.06.06 14:00 LYY

- Modify `generateUI.py`, now more fancy. You can adjust theme.

## 24.06.06 12:00 XQC

- Modify style transfer. Add `--attn` parameter. 

## 24.06.06 11:30 XQC

- Add a `--aams` parameter to style transfer. If setting `--aams -l_o`, an aams model will be used, which provides a  more artistic style transfer, since it has self-attention in it.

- Using this model needs to install `modelscope` and `tensorflow`. 

## 24.06.06 10:30 LYY

- Modify `generateAPI.py` and `generateUI.py`. Now more user-friendly.

- Fix serveral small bugs

- Exists problem: Style Transfer seems like no use in the final result. 

## 24.06.06 3:00 XQC

- Add attention to style transfer. Though weird composition, it reduces the negative effect of the style transfer.

## 24.06.05 21:00 LYY

- Add `main.py`, now can run batch of music

## 24.06.05 8:00 LYY

- Put prompt into files in `code`

- Try flashattention, install takes serveral hours but only 10 seconds speed up.

## 24.06.02 18:50 SQA

- Test after restarting a pod, seems to be a success.

- Add a line in `main.ipynb` to clear the directory `style_transfer/` before running the code.

## 24.06.02 17:00 SQA

- Fix a bug from LYY. Modify a bit Qwen's prompt.

- To fix Qwen's lyrics, my only way is to add a parameter `--ignore_lyrics` (default False). In `main.ipynb` it is set to be True and you may change it.

## 24.06.02 13:45 XQC

- Update style images and descriptions. Retribution -> Broken Sky, After Dawn -> Luminescence.

## 24.06.01 21:16 SQA

- Do some prompt engineering. Find two big problems:

1. The style image is not good enough, two of them have low resolution. We need to change them. I collect some good images waiting for you to process.

2. Qwen is very not stable, but is somehow "stable" by outputing the same non-sense lyrics... I think we must try to generate images without the lyrics.


## 24.05.31 9:30 LYY

- Build new docker, really exhausted. Please read the `README.md` for new docker details. I strongly suggest that before building the docker, test the code locally with enough patience.

## 24.05.30 23:00 LYY

- Change `demo/demo.py`, `demo/generateAPI.py`, `demo/generateUI.py`. Now support our new method of generating style image. Set default `num_non_char` and `num_char` to 2, which means 4 photos in total.

- Modify a stupid bug in `process.py`, now binary search correctly.

## 24.05.30 17:30 LYY

- Change the way of generating style image. Instead of directly choose from 20 style descriptions, we use binary search among all the style descriptions to find the most similar one. The result is incredibly stable and more semantically correct.

- Remove some style images.

- Put prompt into individual files. Code becomes more readable.

- Exist problem: during process, model too concentrate on the user input, but not the music. We need to find a way to balance them.

## 24.05.29 22:00 SQA

- modify prompt for non-character image generation.

- During testing I found that the 10th style image (Realms) is bad. Someone should replace it with a better one.

- Qwen is fucking every time. What can I say

## 24.05.29 16:20 SQA

- fix some bugs, now can support multi-prompt generation.

## 24.05.29 9:20 SQA

- fix some bugs, now `main.ipynb` can run without error.

- Note that I modify the output structure in style transfer.

## 24.05.28 22:30 SQA

- Add two parameters (`num_char`, `num_non_char`), corresponding to number of prompts generated by Zhipu.

- Also I mentioned that the version of torch indeed matters, later I will modify requirements.txt.

## 24.05.28 16:30 LYY

- Fix the bug in `llama3.py`, now you can use `llama3-70B` model to generate images. However, I don't think we would use it.
  
- Add prompt, now more robust.

## 24.05.27 23:00 LYY

- Update `MI-T_helm_template`, support up-to 32M music file. 
  
- Make `README.md` more readable.

## 24.05.27 21:30 LYY

- Change the directory structure

- Modify the `README.md`

## 24.05.27 20:30 LYY

-  Finish the docker, now you can run the whole pipeline using docker. Please read the `README.md` for more details. (What a damn hard work!üò≠ I don't want to use docker, reboot system, deal with all strange errors anymore)
  
-  Next step: improve `style_transfer.py`.
  
-  Besides, running the whole pipeline using docker is really slow, a good way is load all the models into memory when the agent starts, and then use them directly. But I'm tired of doing such things. 

-  When doing style_transfer, we need to download vgg-19, it might be a good idea to download it when starting the pod. That's not hard.

## 24.05.27 10:00 LYY

- Add `demo.py` which auxiliary to `generateAPI.py` and `generateUI.py`.

- Modify `style_transfer.py`, the implementation of SQA is not robust enough, can't support multi-picture generation. However, my implementation is not readable enough, if you have difficulty in understanding, please contact me.

- Remove many useless import files, modify the `requirements.txt`, `.gitignore` and `environment.sh`.

- Now, use `generateAPI.py` and `generateUI.py`, you can run our whole pipeline, from music to image, that's a great progress!ü•∞ Please take a look at __`README.md`__ for more details. The next step is improving the robustness of code, especially for the messy directory structure.

- The next step is to improve the robustness of the code and the quality of the generated image.

- New docker waiting to complete.

## 24.05.25 19:35 SQA

- Complete a UI using docker, but still long way to go.

You can use it by first downloading the demo directory to your local machine, then `cd demo/docker/MI-T_helm_template`, and then __remember to change the `NameSpace` in `project.yaml`__. Now you might use the __makefile__ I write for you: (remember to first helm delete your own pod)

```
make podup
make connect
```

The first command will create the API & UI pod, and the second command will connect these two. Then you can use the UI by visiting `http://sqabuhui.ddns.net:9443`.

After using you can do

```
make poddown
```

which will delete the pods and the connection.

- To use your own ip, modify `IngressHost: sqabuhui.ddns.net` in `./project.yaml`.

- The containers I build is all in `harbor.ai.iiis.co:9443/llmproject4sqalyyxqc`, in which there are `/api/try{x}` or `/ui/try{x}` (You see, this cannot be a good name, though.) To modify the code (also the dockerfile or requirments), push your own docker image and modify the `project.yaml` to use your own image.

- The `apiurl: http://api-service:54224/generate` line in `project.yaml`, you had better not modify this unless talk to me, since this is important why this can work.

- Note that in my implementation, I modify lyy's code so that in API, the music will be saved into `MP3_PATH: /ssdshare/MI-T/music/music.mp3` which you may modify. __But actually it should be `.wav`__. (Sorry for the mistake and I will fix it later...) So the expected type will be `.wav`. __TODO:__ implement the rest of our model based on the music saved.

The final part is the biggest problem, which I have not figured out why, in that __one cannot upload a large music in the gradio__ (tested, 10s music is definitely fine, but `Burn` is too large). 

The error is contained in the `error.txt`.

Lyy's implementation does not have this problem, so I guess that the problem is either
1. on the gradio pod there is limitation of uploading
2. there is limitation on `sqabuhui.ddns.net`

That's the end and I love docker so much..


## 24.05.22 18:00 XQC

- Try to figure out why Qwen doesn't work sometimes. Probably related with luck, since this time when I changed back to 4 gpus, it still returned trash second time on the same gpu.


## 24.05.15 13:30 XQC

- Complete style image description. Replace all descriptions with gpt-4o.

## 24.05.13 16:00 XQC

- Add style images. Please `cd codes`, `chmod +x environment.sh`, `./environment.sh` to set the correct environment.

- Modify `style_transfer.py` and `process.py`. 

- TODO : Complete style description.

## 24.05.12 18:00 XQC

- Various fix. Plz `pip install -r requirements.txt` to update the environment.

- Modify style transfer.

## 24.05.12 17:38 SQA

- When trying styles, meet with a problem with Qwen, saying that 

```
AssertionError: Pass argument `stream` to model.chat() is buggy, deprecated, and marked for removal. Please use model.chat_stream(...) instead of model.chat(..., stream=True).
Âêëmodel.chat()‰º†ÂÖ•ÂèÇÊï∞streamÁöÑÁî®Ê≥ïÂèØËÉΩÂ≠òÂú®BugÔºåËØ•Áî®Ê≥ïÂ∑≤Ë¢´Â∫üÂºÉÔºåÂ∞ÜÂú®Êú™Êù•Ë¢´ÁßªÈô§„ÄÇËØ∑‰ΩøÁî®model.chat_stream(...)‰ª£Êõømodel.chat(..., stream=True)„ÄÇ
```

Had better give it to XQC to fix.

- Modify the prompt to contain more information, meaning that the prompt is longer..

## 24.05.09 12:00 LYY

- Add `generateAPI.py` and `generateUI.py`, you can try them: `python generateAPI.py`  to start server, `python generateUI.py` to start UI. (The image generated is only according to the prompt, the description returned is your original prompt. I will change them after finishing project.)

- Test them as much as you can, thanks.

## 24.05.08 19:10 LYY

- Make a tiny change in `llama3.py`, but it still doesn't work (I change it earlier, but forget to push).

- fix SQA's update log.

## 24.05.06 9:26 SQA

- Fix a very small bug in `main.ipynb`. And I have thrown lots of style images in `/ssdshare/style/`.

## 24.05.06 20:30 LYY

- Try to add "llama3-70B" model to `process.py`. However, fail due to some strange errors. Don't call this model temporarily.

- Strange error occurs in `generate.py`, could someone fix it ?

## 24.05.06 16:09 SQA

- Implement style_transfer API. And also modify `style_transfer.py` to make it more runnable.

To specify, I keep the original way to run `style_transfer.py` with argument `-c`, `-s`, but they will be no longer required.

Instead, you can write a file `style_list.txt` in `data`, containing style image names from style library `data/style/` (of course, you must first upload your style image into the library). Style images will be read from this file by default. And content images will be read from `input_list.txt` by default. (Now I only implement `0.png` generated, but later...)

The code will automatically transfer any content to any style. (You still need to add `c_p` if wanted). 

In `main.py` everything is done except it only transfers `0.png` in `.tmp/generate/`. The output will be saved in `.tmp/style_transfer/`.

## 24.05.05 20:27 SQA

- Add prompt for no-character image generation. It seems that the model is rather good at this aspect.

- Also modify some code to be run more conveniently.

- For no-character, prompt will be `<audio>.prompt2`, image will be `<audio>.10-12`.

## 24.05.05 19:30 LYY

- Try Meta-Llama-3-70B-Instruct-AWQ model as process, it needs 4 GPU to run and the output is really slow (4 minutes per question). The reason is the model is too large s.t. 4 GPU is almost full. The output seems to be good, but I haven't run enough tests. I think someone may need to ask TA for at least 6 GPU to test it.

- Add Meta-Llama-3-70B-Instruct-AWQ model to `process.py`.

- Our prompt really needs to be modified !!!

## 24.05.05 15:00 XQC

- Use https://arxiv.org/pdf/1606.05897 to preserve the color of the content image. 

- Modify prompts.

## 24.05.04 20:48 SQA

- Modify some paths to make `extract.py` runnable independently.

## 24.05.04 17:45 XQC

- Find and modify a style transfer, based on Gatys' paper. It's a little bit slow, but the result is good.  This is a standalone implementation, and the interface has not been written yet.

## 24.05.04 16:00 XQC

- Modify several files. Now work properly.

## 24.05.04 14:48 SQA

- Improve prompts in process. Maybe we should try another way.

## 24.05.04 9:23 SQA

- Add input propmt.

To run the code, first create a file `data/input_prompt.txt` and write your prompt here. It can be empty.

## 24.05.03 21:00 LYY

- Create `generate.py`. Now we can generate images using `main.ipynb` from the original music! A memorable milestone!

- We need to find a way to get more information during process step. Because only several words prompt seems to be not enough to generate a good image.

## 24.05.03 17:00 XQC

- Modify `process.py`. File output now.

- Create `main.ipynb` to organize the process.

## 24.05.03 14:31 SQA

- Prompting engineering in `precess.py`. And tiny change in `test_playground.ipynb` in output directory.

## 24.05.03 14:30 LYY

- Change `tryInstantStyle.ipynb`. Don't use stable-diffusion model anymore! The image generated really damages the eyes. However, it works well with the `playground-v2.5-1024px-aesthetic` model.

- Try some examples to generate images using `playground-v2.5-1024px-aesthetic` with long prompts, `lpw_stable_diffusion_xl.py` really works!

- Now can use `stable-diffusion-xl-base-1.0` to generate images with `use_safetensors=True` and `variant="fp16"`. (But don't use it makes better images.)

## 24.05.03 12:30 XQC
- Fix SQA's `tryInstantStyle.ipynb`. We'd better not use this, huh.

## 24.05.03 10:05 SQA
- Throw `tryInstantStyle.ipynb`, which attempts to use InstantStyle but fails. The repo of InstantStyle has been cloned into `/ssdshare/LLMs/InstantStyle`. The original repo url is `https://github.com/InstantStyle/InstantStyle.git`. Note that you should read the README in the repo before you try to run the codes.

- The codes are all from README, but a problem is that it must connect huggingface to access the base model
`stabilityai/stable-diffusion-xl-base-1.0`. I tried to change this into the base model in ssdshare by lyy but failed, raising error saying that something was missing in the base model path. Then I tried to change the `StableDiffusionXLPipeline.from_pretrained` into `DiffusionPipeline.from_pretrained`, which was used in lyy's codes, but more confusing errors occured and I gave up..

- You can also find a demo notebook in the repo. But there is an error `cannot import name 'StableDiffusionXLControlNetPipeline' from 'diffusers'.` in some cell. (It seems that if you have the repo, you don't need to run all the `install dependency` at the beginning of this notebook.)

- Why can't I connect to hugging face???

## 24.05.03 0:45 XQC
- Fix the pipeline `lpw_stable_diffusion_xl.py`. Now load the custom pipeline from local file. Tested it works by asking "background be black" at the very very end of the prompt. This bug is caused by not copying codes completely from the original pipeline. Ah, what can I say.

## 24.05.02 15:00 XQC
- Half of process part.

- Modify `extract.py`, create `data` directory.

## 24.05.02 14:30 LYY

- Try `Stable-Diffusion-xl-based-1.0`

- Try `Stable-Diffusion-xl-based-1.0 + stable-diffusion-xl-refiner-1.0`. With the refiner model, the output is more realistic. However, the refiner model can't able to handle person's hands and other characteristics.

- Try playground-v2.5-1024px-aesthetic, the output is really fantastic. You need to download latest version of diffuser to run. Limits: only 77 tokens for input.

## 24.05.02 11:00 XQC
- Modify `extract.py`, now it accepts multi inputs. But still don't know why sometimes it throws trash.

## 24.04.25 19:30 XQC
- Try Qwen-chat. Ali's Qwen-chat based on Qwen-audio performs well (which doesn't use MERT, better than that of tecent).

- It can conclude the music and extract lyrics, which satisfies all the requirements. The point is that it can only deal with a window of 30s (?not sure), maybe music should be segmented.

## 24.04.24 16:00 XQC
- Try the MERT-v1-330M model. But modelscope deceived me that it outputs text, but it actually outputs vectors. It's hard to deal with these vectors.

- Now there're two ways to go. One is to train or find models dealing with these tensors. The other is to find models music -> text. I think the latter is more feasible.