########### First thing first ###########
NameSpace: s0174   # 自己的namespace （同用户名）

########### API Configuration ###########
# vllmPort: 8000
ContainerImage: harbor.ai.iiis.co:9443/lyy/tryapi19:latest

# Using the llama-3-8B-instruct model
# ModelName: /ssdshare/Meta-Llama-3-8B-Instruct/
MP3_PATH: /ssdshare/MI-T/music/music.mp3

# ModelName: /ssdshare/xuw/llama-2-7b-py18k_merged/

# Using the llama-3-70B-instruct model, sort of runs with the following, and set DshmSize to 16Gi, but not working well
# ModelName: /ssdshare/Meta-Llama-3-70B-Instruct-awq/
# AdditionalArgs: | 
#     "--trust-remote-code", "--quantization", "awq", "--kv-cache-dtype", "fp8", "--tensor-parallel-size", "2", "--max-model-len", "6432", 

# Or using the Phi-3 model - you will need to add two additional arguments
# ModelName: /ssdshare/Phi-3-mini-128k-instruct/
# AdditionalArgs: | 
#     "--trust-remote-code", "--max-model-len", "7260",

# Resource for the vLLM server
Limits:  
 CPU: 16
 memory: 64Gi
 GPU: 2
DshmSize: 4Gi

# Optional: Replace the default Command and Args

# vLLMCommand: '["cat", "code/generateAPI.py"]'
# vLLMArgs: | 
#       ["--model", "/ssdshare/Meta-Llama-3-8B-Instruct/", 
#        "--dtype", "auto",
#        "--api-key", "$VLLM_API_KEY",
#        ]

########### Gradio Configuration ###########

## Replace with your domain name and gradio app docker image
IngressHost: llmlab.ddns.net
GradioImage: harbor.ai.iiis.co:9443/llmproject4sqalyyxqc/ui/base3:latest
apiurl: http://api-service:54224/generate

# OpenAIUrl: http://openai-api:8000/v1
# APIserverURL: 

## Optional: Replace the default Command and Args
# GradioCommand: ["python", "code/generateUI.py"]
# GradioArgs: ["--apiurl", "http://api-service:54224/generate"]

