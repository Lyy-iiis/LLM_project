{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use in diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1588aa7947474630b34e7c6ce97af537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "from PIL import Image\n",
    "\n",
    "from ip_adapter import IPAdapterXL\n",
    "\n",
    "base_model_path = \"/ssdshare/LLMs/stable-diffusion-xl-base-1.0/\"\n",
    "image_encoder_path = \"/ssdshare/LLMs/h94-IP-Adapter/sdxl_models/image_encoder/\"\n",
    "ip_ckpt = \"/ssdshare/LLMs/h94-IP-Adapter/sdxl_models/ip-adapter_sdxl.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "# load SDXL pipeline\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    # add_watermarker=False,\n",
    "    variant=\"fp16\"\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "# reduce memory consumption\n",
    "# pipe.enable_vae_tiling()\n",
    "\n",
    "# load ip-adapter\n",
    "# target_blocks=[\"block\"] for original IP-Adapter\n",
    "# target_blocks=[\"up_blocks.0.attentions.1\"] for style blocks only\n",
    "# target_blocks = [\"up_blocks.0.attentions.1\", \"down_blocks.2.attentions.1\"] # for style+layout blocks\n",
    "ip_model = IPAdapterXL(pipe, image_encoder_path, ip_ckpt, \"cuda:1\", target_blocks=[\"up_blocks.0.attentions.1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def pil_to_cv2(image_pil):\n",
    "    image_np = np.array(image_pil)\n",
    "    image_cv2 = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "    return image_cv2\n",
    "def resize_img(\n",
    "    input_image,\n",
    "    max_side=1280,\n",
    "    min_side=1024,\n",
    "    size=None,\n",
    "    pad_to_max_side=False,\n",
    "    mode=Image.BILINEAR,\n",
    "    base_pixel_number=64,\n",
    "):\n",
    "    w, h = input_image.size\n",
    "    if size is not None:\n",
    "        w_resize_new, h_resize_new = size\n",
    "    else:\n",
    "        ratio = min_side / min(h, w)\n",
    "        w, h = round(ratio * w), round(ratio * h)\n",
    "        ratio = max_side / max(h, w)\n",
    "        input_image = input_image.resize([round(ratio * w), round(ratio * h)], mode)\n",
    "        w_resize_new = (round(ratio * w) // base_pixel_number) * base_pixel_number\n",
    "        h_resize_new = (round(ratio * h) // base_pixel_number) * base_pixel_number\n",
    "    input_image = input_image.resize([w_resize_new, h_resize_new], mode)\n",
    "\n",
    "    if pad_to_max_side:\n",
    "        res = np.ones([max_side, max_side, 3], dtype=np.uint8) * 255\n",
    "        offset_x = (max_side - w_resize_new) // 2\n",
    "        offset_y = (max_side - h_resize_new) // 2\n",
    "        res[\n",
    "            offset_y : offset_y + h_resize_new, offset_x : offset_x + w_resize_new\n",
    "        ] = np.array(input_image)\n",
    "        input_image = Image.fromarray(res)\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', symbolizing the constant fights she has to face. despite the harsh environment, she maintains a confident and determined expression. the background should be black.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', symbolizing the constant fights she has to face. despite the harsh environment, she maintains a confident and determined expression. the background should be black.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d57781a1314c12ad54a95b05351f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b310b58c1724e1fa7a70b97df26c47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8430662905ce40eabd4f14908338d7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Create a detailed and refined image of a woman from the anime Darling in the Franxx. She is known for her distinctive pink hair and mesmerizing green eyes. She should be depicted in a dynamic pose, showcasing her strong and fearless personality. The image should be in anime style, with an 8k resolution and a 16:9 aspect ratio. The background should be a battlefield, symbolizing the constant fights she has to face. Despite the harsh environment, she maintains a confident and determined expression. The background should be black.\"\n",
    "negatives = \"text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\"\n",
    "image = pipe(prompt=prompt).images[0]\n",
    "image.resize((512, 512))\n",
    "image.save(\"image.png\")\n",
    "\n",
    "prompt = \"Ganyu from Genshin Impact, blue hair and blue eyes, horns and a long ponytail, a blue and white outfit with cloud patterns and gold accents, in an elegant pose, standing in front of a scenic background like mountains, a waterfall, graceful, fantastical, 8k resolution, 16:9 aspect ratio, comic style\"\n",
    "\n",
    "\n",
    "input_image = pipe(prompt=prompt,negative_prompt=negatives).images[0]\n",
    "input_image.save(\"input_image.png\")\n",
    "input_image = \"input_image.png\"\n",
    "\n",
    "input_image = Image.open(input_image)\n",
    "if input_image is not None:\n",
    "    input_image = resize_img(input_image, max_side=1024)\n",
    "    cv_input_image = pil_to_cv2(input_image)\n",
    "    detected_map = cv2.Canny(cv_input_image, 50, 200)\n",
    "    canny_map = Image.fromarray(cv2.cvtColor(detected_map, cv2.COLOR_BGR2RGB))\n",
    "else:\n",
    "    canny_map = Image.new('RGB', (1024, 1024), color=(255, 255, 255))\n",
    "    control_scale = 0\n",
    "\n",
    "# generate image variations with only image prompt\n",
    "images = ip_model.generate(pil_image=image,\n",
    "                            prompt=\"a man, masterpiece, best quality, high quality\",\n",
    "                            negative_prompt= \"text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\",\n",
    "                            scale=1.0,\n",
    "                            guidance_scale=5,\n",
    "                            num_samples=1,\n",
    "                            num_inference_steps=60, \n",
    "                            seed=42,\n",
    "                            image=canny_map,\n",
    "                            #neg_content_prompt=\"a rabbit\",\n",
    "                            #neg_content_scale=0.5,\n",
    "                          )\n",
    "\n",
    "images[0].save(\"result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d198fa77a44365a5aff3139494df61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import EDMDPMSolverMultistepScheduler\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "import torch\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"/ssdshare/LLMs/playground-v2.5-1024px-aesthetic/\",\n",
    "    custom_pipeline = \"/root/LLM_project/codes/generate/lpw_stable_diffusion_xl.py\",\n",
    "    # custom_pipeline = \"lpw_stable_diffusion_xl\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5023238e7145b7bdf3cc4724a3cb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c5fa70cb28402bb693817bae5fc022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Create a detailed and refined image of a woman from the anime Darling in the Franxx. She is known for her distinctive pink hair and mesmerizing green eyes. She should be depicted in a dynamic pose, showcasing her strong and fearless personality. The image should be in anime style, with an 8k resolution and a 16:9 aspect ratio. The background should be a battlefield, symbolizing the constant fights she has to face. Despite the harsh environment, she maintains a confident and determined expression. The background should be black.\"\n",
    "negatives = \"text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\"\n",
    "image = pipe(prompt=prompt).images[0]\n",
    "image.resize((512, 512))\n",
    "image.save(\"image.png\")\n",
    "\n",
    "prompt = \"Ganyu from Genshin Impact, blue hair and blue eyes, horns and a long ponytail, a blue and white outfit with cloud patterns and gold accents, in an elegant pose, standing in front of a scenic background like mountains, a waterfall, graceful, fantastical, 8k resolution, 16:9 aspect ratio, comic style\"\n",
    "\n",
    "\n",
    "input_image = pipe(prompt=prompt,negative_prompt=negatives).images[0]\n",
    "input_image.save(\"input_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec22d4a0c5d941a2b0ca14d89ae78b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_image = Image.open(\"input_image.png\")\n",
    "if input_image is not None:\n",
    "    input_image = resize_img(input_image, max_side=1024)\n",
    "    cv_input_image = pil_to_cv2(input_image)\n",
    "    detected_map = cv2.Canny(cv_input_image, 50, 200)\n",
    "    canny_map = Image.fromarray(cv2.cvtColor(detected_map, cv2.COLOR_BGR2RGB))\n",
    "else:\n",
    "    canny_map = Image.new('RGB', (1024, 1024), color=(255, 255, 255))\n",
    "    control_scale = 0\n",
    "\n",
    "# generate image variations with only image prompt\n",
    "images = ip_model.generate(pil_image=image,\n",
    "                            prompt=\"beautiful woman, best quality, high quality\",\n",
    "                            negative_prompt= \"text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\",\n",
    "                            scale=1.0,\n",
    "                            guidance_scale=5,\n",
    "                            num_samples=1,\n",
    "                            num_inference_steps=60, \n",
    "                            seed=42,\n",
    "                            image=canny_map,\n",
    "                            #neg_content_prompt=\"a rabbit\",\n",
    "                            #neg_content_scale=0.5,\n",
    "                          )\n",
    "\n",
    "images[0].save(\"result.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Resolution Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hidiffusion import apply_hidiffusion, remove_hidiffusion\n",
    "\n",
    "# reduce memory consumption\n",
    "pipe.enable_vae_tiling()\n",
    "\n",
    "# apply hidiffusion with a single line of code.\n",
    "apply_hidiffusion(pipe)\n",
    "\n",
    "...\n",
    "\n",
    "# generate image at higher resolution\n",
    "images = ip_model.generate(pil_image=image,\n",
    "                           prompt=\"a cat, masterpiece, best quality, high quality\",\n",
    "                           negative_prompt= \"text, watermark, lowres, low quality, worst quality, deformed, glitch, low contrast, noisy, saturation, blurry\",\n",
    "                           scale=1.0,\n",
    "                           guidance_scale=5,\n",
    "                           num_samples=1,\n",
    "                           num_inference_steps=30, \n",
    "                           seed=42,\n",
    "                           height=2048,\n",
    "                           width=2048\n",
    "                          )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
